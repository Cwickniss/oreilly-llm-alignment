# Alignment and Reinforcement Learning with Large Language Models (LLMs)


![O'Reilly](images/oreilly.png)


This repository contains Jupyter notebooks for the courses ["Aligning Large Language Models"](https://www.oreilly.com/live-events/aligning-large-language-models/0636920098043/0636920098042/) and ["Reinforcement Learning with Large Language Models"](https://www.oreilly.com/live-events/reinforcement-learning-with-large-language-models/0636920095685) by Sinan Ozdemir. Published by Pearson, the course covers effective best practices and industry case studies in using Large Language Models (LLMs).

## Aligning Large Language Models

- In-depth exploration of various alignment techniques with hands-on case studies, such as Constitutional AI
- Comprehensive coverage of evaluating alignment, offering specific tools and metrics for continuous assessment and adaptation of LLM alignment strategies
- A focus on ethical considerations and future directions, ensuring participants not only understand the current landscape but are also prepared for emerging trends and challenges in LLM alignment

This class is an intensive exploration into the alignment of Large Language Models (LLMs), a vital topic in modern AI development. Through a combination of theoretical insights and hands-on practice, participants will be exposed to various alignment techniques, including a focus on Constitutional AI, constructing reward mechanisms from human feedback, and instructional alignment. The course will also provide detailed guidance on evaluating alignment, with specific tools and metrics to ensure that models align with desired goals, ethical standards, and real-world applications.

### Course Set-Up

- Jupyter notebooks can be run alongside the instructor, but you can also follow along without coding by viewing pre-run notebooks here.

### Notebooks

- Red Teaming Benchmark [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1JZEjJwhtpnh6QXH9VTn5ub599cpxaEHX?usp=sharing)

- Constitutional AI (CAI) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NPBsE8BdD7W9SyVh44_7KEt-GdiNZqhv?usp=sharing)

- [Prompt Injection Examples](notebooks/prompt_injection.ipynb) - See how three kinds of prompt injection attacks can attempt to jailbreak an LLM

## Reinforcement Learning with Large Language Models

- An immersive deep dive into advanced concepts of reinforcement learning in the context of LLMs.
- A practical, hands-on approach to fine-tuning LLMs, with a focus on real-world applications such as generating neutral summaries using T5.
- A unique opportunity to understand and apply innovative concepts like RLHF, RLAIF, and Constitutional AI in reinforcement learning.

This training offers an intensive exploration into the frontier of reinforcement learning techniques with large language models (LLMs). We will explore advanced topics such as Reinforcement Learning with Human Feedback (RLHF), Reinforcement Learning from AI Feedback (RLAIF), and Constitutional AI, and demonstrate practical applications such as fine-tuning open source LLMs like FLAN-T5 and Llama-3. This course is critical for those keen on deepening their understanding of reinforcement learning, its latest trends, and its application to LLMs.

### Course Set-Up

- Jupyter notebooks can be run alongside the instructor, but you can also follow along without coding by viewing pre-run notebooks here.

### Notebooks

- FLAN-T5 PPO - Working with FLAN-T5 models using Reinforcement Learning [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1wG8lv6drn872HNZHrT7V9kl6JIF1SXpr?usp=sharing)

- Reward Modeling - Training a reward model from human preferences [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bVjTzOjXCOM8J6tzgt3LK-D0K-yGWzyI?usp=sharing)

- DPO - Direct Preference Optimization [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1mi0yF_wmbn4U7pYkh7WCDdkHD2h7UJxR?usp=sharing)

- RLOO - Reinforcement Learning with Leave-One-Out [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1_htV72wgados0kCD2xrKTR6aTOAtqTj7?usp=sharing)

- GRPO - Fine-tuning with Group Relative Policy Optimization [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Cws1IL_T_0_cP0-cHxFA0FEsXYdiAN_8?usp=sharing)

## Further Resources

- [Other Useful Links](https://learning.oreilly.com/playlists/2953f6c7-0e13-49ac-88e2-b951e11388de/)
