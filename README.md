# Reinforcement Learning with Large Language Models


![O'Reilly](images/oreilly.png)

## Description

This repository contains Jupyter notebooks for the course ["Reinforcement Learning with Large Language Models"](https://learning.oreilly.com/live-events/llms-from-prototypes-to-production/0636920095639/) by Sinan Ozdemir. Published by Pearson, the course covers effective best practices and industry case studies in using Large Language Models (LLMs).

- An immersive deep dive into advanced concepts of reinforcement learning in the context of LLMs.
- A practical, hands-on approach to fine-tuning LLMs, with a focus on real-world applications such as generating neutral summaries using T5.
- A unique opportunity to understand and apply innovative concepts like RLHF, RLAIF, and Constitutional AI in reinforcement learning.

This training offers an intensive exploration into the frontier of reinforcement learning techniques with large language models (LLMs). We will explore advanced topics such as Reinforcement Learning with Human Feedback (RLHF), Reinforcement Learning from AI Feedback (RLAIF), and Constitutional AI, and demonstrate practical applications such as fine-tuning open source LLMs like FLAN-T5 and GPT-2. This course is critical for those keen on deepening their understanding of reinforcement learning, its latest trends, and its application to LLMs.

## Table of Contents

1. [Course Set-Up](#course-set-up)
2. [Notebooks](#notebooks)
3. [Prerequisites](#prerequisites)
4. [Schedule](#schedule)
5. [Resources](#resources)

## Course Set-Up

- Jupyter notebooks can be run alongside the instructor, but you can also follow along without coding by viewing pre-run notebooks here.

### Prerequisites

- Proficiency in Python programming
- A solid understanding of basic machine learning concepts
- Familiarity with the fundamentals of reinforcement learning and natural language processing

## Notebooks

- `rl_flan_t5_summaries.ipynb`: [Working with FLAN-T5 models using Reinforcement Learning](notebooks/rl_flan_t5_summaries.ipynb)
- `sawyer_1_instruction_ft.ipynb`: [Fine-tuning the instruction model for the SAWYER bot](notebooks/8_sawyer_1_instruction_ft.ipynb).
- `sawyer_2_train_reward_model.ipynb`: [Training a reward model for the SAWYER bot from human preferences.](notebooks/sawyer_2_train_reward_model.ipynb)
- `sawyer_3_rl.ipynb`: [Using Reinforcement Learning from Human Feedback (RLHF) to further align the SAWYER bot](notebooks/sawyer_3_rl.ipynb)
- `sawyer_4_use_sawyer.ipynb`: [Using our SAWYER bot](notebooks/sawyer_4_use_sawyer.ipynb)


For a detailed schedule, refer to the [Course Description](#description).

## Resources

- [Other Useful Links](https://learning.oreilly.com/playlists/2953f6c7-0e13-49ac-88e2-b951e11388de/)
